Mahmoud Jahanshahi | https://mahmoudjahanshahi.com
________________


Profile
Experienced Research/Data Scientist with a demonstrated track record in Empirical Software Engineering research. Adept at applying Machine Learning (ML), Deep Learning, and Natural Language Processing (NLP) to complex, real-world problems. Strong foundation in Statistics and Operations Research (OR/Optimization) to uncover insights and guide data-driven decisions. Recognized for key competencies including:
- Excel at rapidly learning and applying new tools, concepts, and techniques.
- Tackle complex, intellectually demanding problems with analytical depth and minimal guidance.
- Thrive in cross-functional teams and clearly communicate complex ideas across technical and non-technical audiences.
- Take initiative in designing and leading original research projects, from problem formulation to publication.

Education
Ph.D. | Computer Science | May 2021 - May 2025
University of Tennessee, Knoxville, USA
- Dissertation: Copy-Based Reuse and its Implications in Open Source Software Supply Chains.
- Advisor: Dr. Audris Mockus
- GPA: 4.00/4
Master of Science | Industrial Engineering | Sep 2011 - Sep 2013
Sharif University of Technology, Tehran, Iran
- Thesis: The Influence of Information Presentation and Risk Attitude on Asset Allocation in Financial Markets.
- Advisor: Dr. S. T. Akhavan Niaki
Bachelor of Science | Industrial Engineering | Jan 2007 - Jul 2011
Mazandaran Institute of Technology, Babol, Iran
- Ranked 6th nationwide in the Industrial Engineering Graduate Admissions Exam.

Awards
- LLM4Code Best Paper Award for “Cracks in The Stack: Hidden Vulnerabilities and Licensing Risks in LLM Pre-Training Datasets” at the International Workshop on Large Language Models for Code (LLM4Code). 2025.
- ACM SIGSOFT Distinguished Paper Award for “Understanding the Response to Open-Source Dependency Abandonment in the npm Ecosystem” at the International Conference on Software Engineering (ICSE). 2025. 

Certificates
Deep Learning Specialization by DeepLearning.AI on Coursera | Dec 2024
1. Structuring Machine Learning Projects, 2. Neural Networks and Deep Learning, 3. Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization, 4. Convolutional Neural Networks, 5. Sequence Models

Skills
- Programming Languages | Bash (Scripting), R, Python, C
- AI/ML | TensorFlow, Keras, PyTorch, Scikit-learn, NumPy, Pandas, NLTK
- Generative AI | Large Language Models (LLMs), Transformer Architectures, Text Generation, GPT
- Data Analysis | SQL, MongoDB, Tableau, Power BI, MATLAB, Matplotlib, Seaborn, ggplot2
- Big Data | Hadoop, Apache Spark
- DevOps & Productivity | AWS, Azure, GCP, Git, Docker, Jira, MS Project, COMFAR
- Languages | English (Fluent), German (Working Knowledge), Persian (Native)

Professional Experience
Graduate Research Assistant | University of Tennessee Knoxville | May 2021 - May 2025
- Built large-scale OSS analysis pipelines (Bash, Python, R) using ML and NLP (e.g., Winnowing) to detect 24B+ copy instances across 1B copied files from 130M+ projects. Enabled the first ecosystem-scale measurement of copy-based reuse, impacting 80% of OSS, with implications for security, licensing, and maintenance.
- Developed an automated LLM dataset curation pipeline that flagged 19,944 vulnerable code blobs linked to 6,947 distinct CVEs in “The Stack”. Found 17% of files with newer, fixed versions (patched or bug-free), improving LLM pre-training data safety, compliance, and maintainability.
- Contributed core functionality to the World of Code infrastructure, enabling blob-level provenance tracking and metadata analysis across 16B files and 108M de-forked OSS projects. Produced key research outputs including a copy-based reuse dataset, project-to-license map, and an interactive author/project collaboration network tool.
Senior Data Scientist | Mobile Communications Company of Iran | May 2019 - Apr 2020
- At the ERP department, data warehousing & business intelligence office, bridged the gap between business units and data warehouse teams by acting as a liaison, translating business needs into technical requirements for the design, testing, and development of data warehouse solutions.
- Developed and maintained reports, dashboards, and analyses using Oracle Business Intelligence Enterprise (OBIEE) and SQL, leveraging ETL pipelines to extract, transform and analyze enterprise-scale datasets for decision support.
Strategic Investments Lead | Mobile Communications Company of Iran | Feb 2018 - May 2019
- At the mergers and acquisitions department, feasibility studies office, led a team of five professionals in managing complex investment projects, including business plan development, feasibility analysis, valuation of acquisition targets and due diligence.
- Collaborated with C-level management and the board of directors to negotiate contract terms and cooperation models with potential counterparts, such as strategic partners or  business sellers.
International Investment Analyst | Mobile Communications Company of Iran | Feb 2016 - Feb 2018
- At the mergers and acquisitions department, international investments office, developed financial models to project earnings and assess the viability of acquisition opportunities in international markets.
- Screened markets for potential investment targets, interpreting financial statements and other data to evaluate risk and profitability.

Publications
- Jahanshahi M., Mockus A. “Cracks in The Stack: Hidden Vulnerabilities and Licensing Risks in LLM Pre-Training Datasets”. Accepted in the Second International Workshop on Large Language Models for Code (LLM4Code). 2025. Won the LLM4Code Best Paper Award. Abstract: A critical part of creating code suggestion systems is the pre-training of Large Language Models on vast amounts of source code and natural language text, often of questionable origin or quality. This may contribute to the presence of bugs and vulnerabilities in code generated by LLMs. While efforts to identify bugs at or after code generation exist, it is preferable to pre-train or fine-tune LLMs on curated, high-quality, and compliant datasets. The need for vast amounts of training data necessitates that such curation be automated, minimizing human intervention. We propose an automated source code autocuration technique that leverages the complete version history of open-source software projects to improve the quality of training data. This approach leverages the version history of all OSS projects to identify training data samples that have been modified or have undergone changes in at least one OSS project, and pinpoint a subset of samples that include fixes for bugs or vulnerabilities. We evaluate this method using The Stack v2 dataset, and find that 17% of the code versions in the dataset have newer versions, with 17% of those representing bug fixes, including 2.36% addressing known CVEs. The deduplicated version of Stack v2 still includes blobs vulnerable to 6,947 known CVEs. Furthermore, 58% of the blobs in the dataset were never modified after creation, suggesting they likely represent software with minimal or no use. Misidentified blob origins present an additional challenge, as they lead to the inclusion of non-permissively licensed code, raising serious compliance concerns. By addressing these issues, the training of new models can avoid perpetuating buggy code patterns or license violations. We expect our results to inspire process improvements for automated data curation, with the potential to enhance the reliability of outputs generated by AI tools.
- Jahanshahi, M., Reid, D., & Mockus, A. “Beyond Dependencies: The Role of Copy-Based Reuse in Open Source Software Development”. Accepted in ACM Transactions on Software Engineering and Methodology (TOSEM). 2025. Abstract: In Open Source Software, resources of any project are open for reuse by introducing dependencies or copying the resource itself. In contrast to dependency-based reuse, the infrastructure to systematically support copy-based reuse appears to be entirely missing. Our aim is to enable future research and tool development to increase efficiency and reduce the risks of copy-based reuse. We seek a better understanding of such reuse by measuring its prevalence and identifying factors affecting the propensity to reuse. To identify reused artifacts and trace their origins, our method exploits World of Code infrastructure. We begin with a set of theory-derived factors related to the propensity to reuse, sample instances of different reuse types, and survey developers to better understand their intentions. Our results indicate that copy-based reuse is common, with many developers being aware of it when writing code. The propensity for a file to be reused varies greatly among languages and between source code and binary files, consistently decreasing over time. Files introduced by popular projects are more likely to be reused, but at least half of reused resources originate from “small” and “medium” projects. Developers had various reasons for reuse but were generally positive about using a package manager.
- Jahanshahi, M., Reid, D., McDaniel, A., & Mockus, A. “OSS License Identification at Scale: A Comprehensive Dataset Using World of Code”. Accepted in 2025 IEEE/ACM 22st International Conference on Mining Software Repositories (MSR). IEEE, 2025. Abstract: The proliferation of open source software (OSS) and different types of reuse has made it incredibly difficult to perform an essential legal and compliance task of accurate license identification within the software supply chain. This study presents a reusable and comprehensive dataset of OSS licenses, created using the World of Code (WoC) infrastructure. By scanning all files containing "license" in their file paths, and applying the approximate matching via winnowing algorithm to identify the most similar license from the SPDX list, we found and identified 5.5 million distinct license blobs in OSS projects. The dataset includes a detailed project-to-license (P2L) map with commit timestamps, enabling dynamic analysis of license adoption and changes over time. To verify the accuracy of the dataset we use stratified sampling and manual review, achieving a final accuracy of 92.08%, with precision of 87.14%, recall of 95.45%, and an F1 score of 91.11%. This dataset is intended to support a range of research and practical tasks, including the detection of license noncompliance, the investigations of license changes, study of licensing trends, and the development of compliance tools. The dataset is open, providing a valuable resource for developers, researchers, and legal professionals in the OSS community.
- Miller, C., Jahanshahi, M., Mockus, A., Vasilescu, B., & Kästner, C. “Understanding the Response to Open-Source Dependency Abandonment in the npm Ecosystem”. Accepted in the 47th International Conference on Software Engineering (ICSE). 2025. Won the ACM SIGSOFT Distinguished Paper Award. Abstract: Many developers relying on open-source digital infrastructure expect continuous maintenance, but even the most critical packages can become unmaintained. Despite this, there is little understanding of the prevalence of abandonment of widely-used packages, of subsequent exposure, and of reactions to abandonment in practice, or the factors that influence them. We perform a large-scale quantitative analysis of all widely-used npm packages and find that abandonment is common among them, that abandonment exposes many projects which often do not respond, that responses correlate with other dependency management practices, and that removal is significantly faster when a package’s end-of-life status is explicitly stated. We end with recommendations to both researchers and practitioners who are facing dependency abandonment or are sunsetting packages, such as opportunities for low-effort transparency mechanisms to help exposed projects make better, more informed decisions.
- Thakur, A., Milewicz, R., Jahanshahi, M., Paganini, L., Vasilescu, B., & Mockus, A. “Scientific Open-Source Software Is Less Likely To Become Abandoned Than One Might Think! Lessons from Curating a Catalog of Maintained Scientific Software”. Accepted in The ACM International Conference on the Foundations of Software Engineering (FSE). 2025. Abstract: Scientific software is essential to scientific innovation and in many ways it is distinct from other types of software. Abandoned (or unmaintained), buggy, and hard to use software, a perception often associated with scientific software can hinder scientific progress, yet, in contrast to other types of software, its longevity is poorly understood. Existing data curation efforts are fragmented by science domain and/or are small in scale and lack key attributes. We use large language models to classify public software repositories in World of Code into distinct scientific domains and layers of the software stack, curating a large and diverse collection of over 18,000 scientific software projects. Using this data, we estimate survival models to understand how the domain, infrastructural layer, and other attributes of scientific software affect its longevity. We further obtain a matched sample of non-scientific software repositories and investigate the differences. We find that infrastructural layers, downstream dependencies, mentions of publications, and participants from government are associated with a longer lifespan, while newer projects with participants from academia had shorter lifespan. Against common expectations, scientific projects have a longer lifetime than matched non-scientific open-source software projects. We expect our curated attribute-rich collection to support future research on scientific software and provide insights that may help extend longevity of both scientific and other projects.
- Jahanshahi, M. & Mockus, A. “Dataset: Copy-based Reuse in Open Source Software”. 2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR) (pp. 42-47). IEEE, 2024. Abstract: In Open Source Software, the source code and any other resources available in a project can be viewed or reused by anyone subject to often permissive licensing restrictions. In contrast to some studies of dependency-based reuse supported via package managers, no studies of OSS-wide copy-based reuse exist. This dataset seeks to encourage the studies of OSS-wide copy-based reuse by providing copying activity data that captures whole-file reuse in nearly all OSS. To accomplish that, we develop approaches to detect copy-based reuse by developing an efficient algorithm that exploits World of Code infrastructure: a curated and cross referenced collection of nearly all open source repositories. We expect this data will enable future research and tool development that support such reuse and minimize associated risks.
- Reid, D., Jahanshahi, M., & Mockus, A. “The extent of orphan vulnerabilities from code reuse in open source software”. Proceedings of the 44th International Conference on Software Engineering (ICSE) (pp. 2104-2115). 2022. Nominated for the ACM SIGSOFT Distinguished Paper Award. Abstract: Motivation: A key premise of open source software is the ability to copy code to other open source projects (white-box reuse). Such copying accelerates development of new projects, but the code flaws in the original projects, such as vulnerabilities, may also spread even if fixed in the projects from where the code was appropriated. The extent of the spread of vulnerabilities through code reuse, the potential impact of such spread, or avenues for mitigating risk of these secondary vulnerabilities has not been studied in the context of a nearly complete collection of open source code. Aim: We aim to find ways to detect the white-box reuse induced vulnerabilities, determine how prevalent they are, and explore how they may be addressed. Method: We rely on World of Code infrastructure that provides a curated and cross-referenced collection of nearly all open source software to conduct a case study of a few known vulnerabilities. To conduct our case study we develop a tool, VDiOS, to help identify and fix white-box-reuse-induced vulnerabilities that have been already patched in the original projects (orphan vulnerabilities). Results: We find numerous instances of orphan vulnerabilities even in currently active and in highly popular projects (over 1K stars). Even apparently inactive projects are still publicly available for others to use and spread the vulnerability further. The often long delay in fixing orphan vulnerabilities even in highly popular projects increases the chances of it spreading to new projects. We provided patches to a number of project maintainers and found that only a small percentage accepted and applied the patch. We hope that VDiOS will lead to further study and mitigation of risks from orphan vulnerabilities and other orphan code flaws.
- Lyulina, E., & Jahanshahi, M. “Building the collaboration graph of open-source software ecosystem”. 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR) (pp. 618-620). IEEE, 2021. Abstract: The Open-Source Software community has become the center of attention for many researchers, who are investigating various aspects of collaboration in this extremely large ecosystem. Due to its size, it is difficult to grasp whether or not it has structure, and if so, what it may be. Our hackathon project aims to facilitate the understanding of the developer collaboration structure and relationships among projects based on the bi-graph of what projects developers contribute to by providing an interactive collaboration graph of this ecosystem, using the data obtained from World of Code [1] infrastructure. Our attempts to visualize the entirety of projects and developers were stymied by the inability of the layout and visualization tools to process the exceedingly large scale of the full graph. We used WoC to filter the nodes (developers and projects) and edges (developer contributions to a project) to reduce the scale of the graph that made it amenable to an interactive visualization and published the resulting visualizations. We plan to apply hierarchical approaches to be able to incorporate the entire data in the interactive visualizations and also to evaluate the utility of such visualizations for several tasks.
